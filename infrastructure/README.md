
If you're setting up your home system, or upgrading your exisiting one, understanding the importance of good infrastructure is key to reduce your maintenance and configuration time, make sure your precious data is safe from critical loss, that both it and your services are secure from prying eyes or malicious agents. Present in any cloud providers service these aspects are to some degree taken care of, it is bundled into the price, so when we think we can save money and effort to do things ourselves, we will follow three deployment topologies, the monolith, primary and secondary, and "serverless" as discussed here.

Shortly summarized, when we lease 1TB of cloud storage, we are also promised a degree of assurance that both the transaction and data is encrypted, and the data is duplicated and divided. Not even if a whole server rack goes down is your data in danger, comparatively using your personal computer would be at risk if that single machine or disk has a critical fault. Another notable difference is related to networking, on a local area network (LAN) the bandwidth and latency you could expect from your server to your clients is a lot better than from a server farm somewhere else, but outside your own network the upload speeds from your gateway is bottlenecked by your ISP, and commecrial solutions may provide better results. Setting up the right infrastructure is not a straight forward thing, we need to know our computational user habits to decide how we scale up and which of the three typologies is right for us.

## Managing Data

Losing data from disk errors or system fault can be anything from re-installing the OS on a thin client (no files locally) or the external drive with all the family photos, anyone who has been unlucky enough to permanently loose precious memories or documents know the importance of backing up, but what 'safe enough'?
Layering the data from the system, then each application individually, how and when they change differ:
- system: data that only changes once the host itself updates OS or drivers etc.
- application data: the databases, keys and other configurations that represents our service in working condition
- user data: data edited or generated by users that cannot be replaced or repaired upon loss

For a monolith structure, where we run everything on one node, we might want to provide a second disk, at least this way if a single disk faults, we have a backup, another solution is to encrypt, compress and store it on a free-tier cloud platform (eg. google drive provides 15gb free, provided compression at 60%-70% of original size it is 19.5 to 21 gb of total storage for your most essential files.)

However this poses one difficulty, if one had mirrored the drive (like RAID 1) then we get away with just changing boot disks and starting recovery, this would have to be done manually at bios. Another option is to set up a secondary node, it's job is backing up your primary node, it is still as many disks, but these partners may recover eachother without us getting our hands dirty in a bios, they can be kept truly headless.

Adding either a new node remotely, or using the cloud solution for user data we have reached the 1-2-3 of backups, there is at leas three layers of duplicity over three location (primary, secondary, remote).


### Bareos

Backing up files and documents is quite different from live applications with databases, since they can change while we copy them the copies risk becoming corrupt, this is where bareos lets us easily manage backing up applications, clients and servers. It consist of three service stacks, the director runs all its dependencies including the web-ui, you usually only need one, and it can be run on the secondary following the topologies from above. The file-daemon is run on the node you want to back up, so probably primary if not both, the storage-deamon runs on nodes you want to back up to, it is however recommended to read up on bareos here.

Besides system malfunctions, another reason for dataloss is user negligence or carelessness, we might delete files by accident, save over important documents etc, a good backup solution should manage both, in bareos we do this by providing incremental and differential backups in addition to full snapshots. If files are deleted, they are still saved in in the history of increments, Apples time machine is a commercial and user friendly application of this for their macs.

To make it easy to configure your own setup you will see the bareos subfolder containing `docker-fileset.conf` this one can be left as is, it tells the system to back up everythin under this folder, in the docker compose, this folder is mounted to the path specified. The director, file and storage daemon come with their own conf examples too, this includes how to use both LAN and VPN (as we look at soon) to connect to our backup services. Each of these configurations must be stored in their respective subfolder of `project_root/bareos/config/director/bareos-dir.d` 

Coming features:
- Encrypt and compress backups
- Jobs and Schedules
- Proper data seperation

caption: Here we also see the nodes with more separation of responsibilities, to ensure a safe snapshot is saved in case of failure both nodes create backups, the same amount of user data is put aside on a commercial cloud for critical files.


#### RAID and LVM

One way to assure duplicity is using RAID configurations, it can additionally increase read and write performance as we distribute over a set of disks. It is a viable solution where we need both very large storage volumes with as high read and write speeds as possible, with the most proficient setups requering both 4 and 5 'identical' disks it is to be considered a specialised solution seldom needed by end consumers. Using linux LVM (logic volume manager) may be the perfect tool for us instead, as we can connect all the old disks and drives we have and combine their storage easily into one logical volume, this is cheap and flexible.

## Availability

As we have seen with the dual node topology we can use one node to restore the other, while this greatly reduces the downtime of our services we might be able to do better, since we are taking a backup to the secondary node, if it had enough overhead it could run the most essential services while restoring the primary, the data is already there. 
 

The third topology will enable scaling out as we please too, this will be done in future releases where the project use kubernetes to divide tasks automatically between our nodes.

### Networking

Since we run our services containerized, we do not need to expose the ports to the host, let alone the gateway, this offers us a very granular level of control to where and how our services can be interacted with. By default each service expose relevant ports from the container to the host, this means that all our services will work on devices connected to the same local area network (LAN), usually our home network. This is a relatively safe option, but it does mean that we cannot access our services our their data from any other network, it is perfectly adequate for some sensitive services, but for our documents and file syncronisation we might want higher availability.

#### VPN

Virtual Private Networks is a conceptually easy and safe way to gain access to your devices wherever you are, as though you were still connected to the same LAN. While your real LAN consist of a full OSI stack from physical to application layer, a VPN is purely an application level network, providers such as ZeroTier offer us a centralized look-up-table that lets us resolve the address to a given node that is also registered on the same virtual network. As long as we keep the network id, and our account credential safe, publicly sharing your VPN IP has little implication on security, it is a great option for most users, but it lacks some usefull features; we cannot share links with any node not on registered to our VPN, so no casting content to a chromecast, no sharing a link to friends and colleagues, for the same reason many public webhook services cannot reach your instance. Another great thing about VPNS is that a single machine can be part of multiple networks, each can be isolated to provide a closed channel for a subset of devices.

For now zerotier is the only service that runs directly on the host, it is a feature currently under development.

Instructions:
- install zerotier on the host: 
  - `curl -s https://install.zerotier.com | sudo bash` 
  - Or alternatives here: [ZeroTier download](https://www.zerotier.com/download/)
- log in to zerotier
  - create network

These commands should now work, using the network id from above try using these commands to join

`
# Get your ZeroTier address and check the service status
sudo zerotier-cli status

# Join, leave, and list networks
sudo zerotier-cli join ################
sudo zerotier-cli leave ################
sudo zerotier-cli listnetworks
`
On the web page you should now see the device trying to connect, accept it.

Mobile devices and desktops have client apps that you repeat the add and accept process with, their IP is reachable for all devices on the same virtual network.

#### Reverse proxy

This is no different than what most public web apps use as well, we forward the 443 (secure http(s)) port from our gateway to our node running the reverse proxy, we can then configure subdomains `subdomain.mydomain.org` or subfolders `mydomain.org/subfolder` that points to the other services we have. Since the communication over the unsafe open networks is secure, we often let the communication from our reverse proxy to our individual services remain on http without affecting our security, to increase general security the SWAG stack offers a set of options for added authentication.

In addition to SWAG, this stack by default comes with duckdns renewer that lets us get `domain.duckdns.org` for free that will point to our public IP, eg to our server.

It is pertinent to read the setup and configuration with care before enabling this with sensitive services as malicious users may try to compromise your node now that they can reach it, but as long as we follow these steps and have otherwise good password and social security habits we should be safe enough; we aren't hiding wikileaks or any high profile data.

#### Combining networks

As they all offer different degrees of availability and security, it might be a good option to use them all, but for different tasks, the services that I want to share, or that needs to be reached from public servers are available through the reverse proxy. More sensitive services such as backing up remotely etc is handled on isolated VPNs to ensure that neither node, transit or location of the data is exposed to unwanted agents.
